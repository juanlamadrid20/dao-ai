"""
LRU (Least Recently Used) cache implementation for Genie SQL queries.

This module provides an in-memory LRU cache that stores SQL queries generated
by Genie. On cache hit, the cached SQL is re-executed against the warehouse
to return fresh data while avoiding the Genie NL-to-SQL translation cost.
"""

from collections import OrderedDict
from datetime import datetime, timedelta
from threading import Lock
from typing import Any

import mlflow
import pandas as pd
from databricks.sdk import WorkspaceClient
from databricks.sdk.service.sql import StatementResponse, StatementState
from databricks_ai_bridge.genie import GenieResponse
from loguru import logger
from mlflow.entities import SpanType

from dao_ai.config import GenieLRUCacheParametersModel, WarehouseModel
from dao_ai.genie.cache.base import (
    CacheResult,
    GenieServiceBase,
    SQLCacheEntry,
)


class LRUCacheService(GenieServiceBase):
    """
    LRU caching decorator that caches SQL queries and re-executes them.

    This service caches the SQL query generated by Genie (not the result data).
    On cache hit, it re-executes the cached SQL using the provided warehouse
    to return fresh data while avoiding the Genie NL-to-SQL translation cost.

    Example:
        from dao_ai.config import GenieLRUCacheParametersModel, WarehouseModel
        from dao_ai.genie.cache import LRUCacheService

        cache_params = GenieLRUCacheParametersModel(
            warehouse=warehouse_model,
            capacity=100,
            time_to_live_seconds=86400  # 24 hours
        )
        genie = LRUCacheService(
            impl=GenieService(Genie(space_id="my-space")),
            parameters=cache_params
        )

    Thread-safe: Uses a lock to protect cache operations.
    """

    impl: GenieServiceBase
    parameters: GenieLRUCacheParametersModel
    name: str
    _cache: OrderedDict[str, SQLCacheEntry]
    _lock: Lock

    def __init__(
        self,
        impl: GenieServiceBase,
        parameters: GenieLRUCacheParametersModel,
        name: str | None = None,
    ) -> None:
        """
        Initialize the SQL cache service.

        Args:
            impl: The underlying GenieServiceBase to delegate to on cache miss
            parameters: Cache configuration including warehouse, capacity, and TTL
            name: Name for this cache layer (for logging). Defaults to class name.
        """
        self.impl = impl
        self.parameters = parameters
        self.name = name if name is not None else self.__class__.__name__
        self._cache = OrderedDict()
        self._lock = Lock()

    @property
    def warehouse(self) -> WarehouseModel:
        """The warehouse used for executing cached SQL queries."""
        return self.parameters.warehouse

    @property
    def capacity(self) -> int:
        """Maximum number of SQL queries to cache."""
        return self.parameters.capacity

    @property
    def time_to_live(self) -> timedelta:
        """Duration after which cached queries expire."""
        return timedelta(seconds=self.parameters.time_to_live_seconds)

    @staticmethod
    def _normalize_key(question: str) -> str:
        """Normalize the question to create a consistent cache key."""
        return question.strip().lower()

    def _is_expired(self, entry: SQLCacheEntry) -> bool:
        """Check if a cache entry has exceeded its TTL."""
        age: timedelta = datetime.now() - entry.created_at
        return age > self.time_to_live

    def _evict_oldest(self) -> None:
        """Remove the oldest (least recently used) entry."""
        if self._cache:
            oldest_key: str = next(iter(self._cache))
            del self._cache[oldest_key]
            logger.debug(f"[{self.name}] Evicted: {oldest_key[:50]}...")

    def _get(self, key: str) -> SQLCacheEntry | None:
        """Get from cache, returning None if not found or expired."""
        if key not in self._cache:
            return None

        entry: SQLCacheEntry = self._cache[key]

        if self._is_expired(entry):
            del self._cache[key]
            logger.debug(f"[{self.name}] Expired: {key[:50]}...")
            return None

        self._cache.move_to_end(key)
        return entry

    def _put(self, key: str, response: GenieResponse) -> None:
        """Store SQL query in cache, evicting if at capacity."""
        if key in self._cache:
            del self._cache[key]

        while len(self._cache) >= self.capacity:
            self._evict_oldest()

        self._cache[key] = SQLCacheEntry(
            query=response.query,
            description=response.description,
            conversation_id=response.conversation_id,
            created_at=datetime.now(),
        )
        logger.info(
            f"[{self.name}] Stored cache entry: key='{key[:50]}...' "
            f"sql='{response.query[:50] if response.query else 'None'}...' "
            f"cache_size={len(self._cache)}/{self.capacity}"
        )

    @mlflow.trace(name="execute_cached_sql", span_type=SpanType.TOOL)
    def _execute_sql(self, sql: str) -> pd.DataFrame | str:
        """
        Execute SQL using the warehouse and return results as DataFrame.

        Args:
            sql: The SQL query to execute

        Returns:
            DataFrame with results, or error message string
        """
        # Log SQL to trace span
        mlflow.update_current_trace(
            tags={
                "cache_layer": self.name,
                "warehouse_id": str(self.warehouse.warehouse_id),
            }
        )

        w: WorkspaceClient = self.warehouse.workspace_client
        warehouse_id: str = str(self.warehouse.warehouse_id)

        logger.debug(f"[{self.name}] Executing cached SQL: {sql[:100]}...")

        statement_response: StatementResponse = w.statement_execution.execute_statement(
            statement=sql,
            warehouse_id=warehouse_id,
            wait_timeout="30s",
        )

        # Poll for completion if still running
        while statement_response.status.state in [
            StatementState.PENDING,
            StatementState.RUNNING,
        ]:
            statement_response = w.statement_execution.get_statement(
                statement_response.statement_id
            )

        if statement_response.status.state != StatementState.SUCCEEDED:
            error_msg: str = f"SQL execution failed: {statement_response.status}"
            logger.error(f"[{self.name}] {error_msg}")
            return error_msg

        # Convert to DataFrame
        if statement_response.result and statement_response.result.data_array:
            columns: list[str] = []
            if statement_response.manifest and statement_response.manifest.schema:
                columns = [
                    col.name for col in statement_response.manifest.schema.columns
                ]
            elif hasattr(statement_response.result, "schema"):
                columns = [col.name for col in statement_response.result.schema.columns]

            data: list[list[Any]] = statement_response.result.data_array
            if columns:
                return pd.DataFrame(data, columns=columns)
            else:
                return pd.DataFrame(data)

        return pd.DataFrame()

    def ask_question(
        self, question: str, conversation_id: str | None = None
    ) -> GenieResponse:
        """
        Ask a question, using cached SQL query if available.

        On cache hit, re-executes the cached SQL to get fresh data.
        Implements GenieServiceBase for seamless chaining.
        """
        result: CacheResult = self.ask_question_with_cache_info(
            question, conversation_id
        )
        return result.response

    @mlflow.trace(name="genie_lru_cache_lookup", span_type=SpanType.TOOL)
    def ask_question_with_cache_info(
        self,
        question: str,
        conversation_id: str | None = None,
        skip_cache: bool = False,
    ) -> CacheResult:
        """
        Ask a question with detailed cache hit information.

        On cache hit, the cached SQL is re-executed to return fresh data.

        Args:
            question: The question to ask
            conversation_id: Optional conversation ID
            skip_cache: If True, bypass this cache (still queries downstream)

        Returns:
            CacheResult with fresh response and cache metadata
        """
        key: str = self._normalize_key(question)

        # Check cache unless skipping
        if not skip_cache:
            with self._lock:
                cached: SQLCacheEntry | None = self._get(key)

            if cached is not None:
                logger.debug(f"[{self.name}] SQL cache hit: {question[:50]}...")

                # Calculate cache entry age
                cache_age: timedelta = datetime.now() - cached.created_at
                cache_age_seconds: float = cache_age.total_seconds()

                # Log cache hit to trace with TTL and timing info
                mlflow.update_current_trace(
                    tags={
                        "cache_hit": "true",
                        "cache_layer": self.name,
                        "cached_sql": cached.query[:200] if cached.query else "",
                        "cache_entry_created_at": cached.created_at.isoformat(),
                        "cache_entry_age_seconds": str(int(cache_age_seconds)),
                        "cache_ttl_seconds": str(
                            int(self.time_to_live.total_seconds())
                        ),
                        "cache_ttl_remaining_seconds": str(
                            int(self.time_to_live.total_seconds() - cache_age_seconds)
                        ),
                    }
                )

                # Re-execute the cached SQL to get fresh data
                result: pd.DataFrame | str = self._execute_sql(cached.query)

                response: GenieResponse = GenieResponse(
                    result=result,
                    query=cached.query,
                    description=cached.description,
                    conversation_id=cached.conversation_id,
                )

                return CacheResult(
                    response=response, cache_hit=True, served_by=self.name
                )

        # Cache miss - delegate to wrapped service
        logger.info(
            f"[{self.name}] Cache {'skip' if skip_cache else 'miss'}: '{question[:50]}...' "
            f"(cache_size={self.size}/{self.capacity}, delegating to {type(self.impl).__name__})"
        )

        # Log cache miss to trace with TTL info
        mlflow.update_current_trace(
            tags={
                "cache_hit": "false",
                "cache_layer": self.name,
                "skip_cache": str(skip_cache),
                "cache_ttl_seconds": str(int(self.time_to_live.total_seconds())),
                "cache_size": str(self.size),
                "cache_capacity": str(self.capacity),
            }
        )

        response = self.impl.ask_question(question, conversation_id)
        with self._lock:
            self._put(key, response)
        return CacheResult(response=response, cache_hit=False, served_by=None)

    def invalidate(self, question: str) -> bool:
        """Remove a specific entry from the cache."""
        key: str = self._normalize_key(question)
        with self._lock:
            if key in self._cache:
                del self._cache[key]
                return True
            return False

    def clear(self) -> int:
        """Clear all entries from the cache."""
        with self._lock:
            count: int = len(self._cache)
            self._cache.clear()
            return count

    @property
    def size(self) -> int:
        """Current number of entries in the cache."""
        with self._lock:
            return len(self._cache)

    def stats(self) -> dict[str, int | float]:
        """Return cache statistics."""
        with self._lock:
            expired: int = sum(1 for e in self._cache.values() if self._is_expired(e))
            return {
                "size": len(self._cache),
                "capacity": self.capacity,
                "ttl_seconds": self.time_to_live.total_seconds(),
                "expired_entries": expired,
                "valid_entries": len(self._cache) - expired,
            }
