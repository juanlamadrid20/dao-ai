"""
Semantic cache implementation for Genie SQL queries using PostgreSQL pg_vector.

This module provides a semantic cache that uses embeddings and similarity search
to find cached queries that match the intent of new questions. Cache entries are
partitioned by genie_space_id to ensure proper isolation between Genie spaces.
"""

from datetime import datetime, timedelta
from typing import Any

import mlflow
import pandas as pd
from databricks.sdk import WorkspaceClient
from databricks.sdk.service.sql import StatementResponse, StatementState
from databricks_ai_bridge.genie import GenieResponse
from loguru import logger
from mlflow.entities import SpanType

from dao_ai.config import (
    DatabaseModel,
    GenieSemanticCacheParametersModel,
    WarehouseModel,
)
from dao_ai.genie.cache.base import (
    CacheResult,
    GenieServiceBase,
    SQLCacheEntry,
)


class SemanticCacheService(GenieServiceBase):
    """
    Semantic caching decorator that uses PostgreSQL pg_vector for similarity lookup.

    This service caches the SQL query generated by Genie along with an embedding
    of the original question. On subsequent queries, it performs a semantic similarity
    search to find cached queries that match the intent of the new question.

    Cache entries are partitioned by genie_space_id to ensure queries from different
    Genie spaces don't return incorrect cache hits.

    On cache hit, it re-executes the cached SQL using the provided warehouse
    to return fresh data while avoiding the Genie NL-to-SQL translation cost.

    Example:
        from dao_ai.config import GenieSemanticCacheParametersModel, DatabaseModel
        from dao_ai.genie.cache import SemanticCacheService

        cache_params = GenieSemanticCacheParametersModel(
            database=database_model,
            warehouse=warehouse_model,
            embedding_model="databricks-gte-large-en",
            time_to_live_seconds=86400,  # 24 hours
            similarity_threshold=0.85
        )
        genie = SemanticCacheService(
            impl=GenieService(Genie(space_id="my-space")),
            parameters=cache_params,
            genie_space_id="my-space"
        )

    Thread-safe: Uses connection pooling from psycopg_pool.
    """

    impl: GenieServiceBase
    parameters: GenieSemanticCacheParametersModel
    genie_space_id: str
    name: str
    _embeddings: Any  # DatabricksEmbeddings
    _pool: Any  # ConnectionPool
    _embedding_dims: int | None
    _setup_complete: bool

    def __init__(
        self,
        impl: GenieServiceBase,
        parameters: GenieSemanticCacheParametersModel,
        genie_space_id: str,
        name: str | None = None,
    ) -> None:
        """
        Initialize the semantic cache service.

        Args:
            impl: The underlying GenieServiceBase to delegate to on cache miss
            parameters: Cache configuration including database, warehouse, embedding model
            genie_space_id: The Genie space ID for partitioning cache entries
            name: Name for this cache layer (for logging). Defaults to class name.
        """
        self.impl = impl
        self.parameters = parameters
        self.genie_space_id = genie_space_id
        self.name = name if name is not None else self.__class__.__name__
        self._embeddings = None
        self._pool = None
        self._embedding_dims = None
        self._setup_complete = False

    def initialize(self) -> "SemanticCacheService":
        """
        Eagerly initialize the cache service.

        Call this during tool creation to:
        - Validate configuration early (fail fast)
        - Create the database table before any requests
        - Avoid first-request latency from lazy initialization

        Returns:
            self for method chaining
        """
        self._setup()
        return self

    def _setup(self) -> None:
        """Initialize embeddings and database connection pool lazily."""
        if self._setup_complete:
            return

        from databricks_langchain import DatabricksEmbeddings

        from dao_ai.memory.postgres import PostgresPoolManager

        # Initialize embeddings
        embedding_model: str = (
            self.parameters.embedding_model
            if isinstance(self.parameters.embedding_model, str)
            else self.parameters.embedding_model.name
        )
        self._embeddings = DatabricksEmbeddings(endpoint=embedding_model)

        # Auto-detect embedding dimensions if not provided
        if self.parameters.embedding_dims is None:
            sample_embedding: list[float] = self._embeddings.embed_query("test")
            self._embedding_dims = len(sample_embedding)
            logger.debug(
                f"[{self.name}] Auto-detected embedding dimensions: {self._embedding_dims}"
            )
        else:
            self._embedding_dims = self.parameters.embedding_dims

        # Get connection pool
        self._pool = PostgresPoolManager.get_pool(self.parameters.database)

        # Ensure table exists
        self._create_table_if_not_exists()

        self._setup_complete = True
        logger.debug(
            f"[{self.name}] Semantic cache initialized for space '{self.genie_space_id}' "
            f"with table '{self.table_name}' (dims={self._embedding_dims})"
        )

    @property
    def database(self) -> DatabaseModel:
        """The database used for storing cache entries."""
        return self.parameters.database

    @property
    def warehouse(self) -> WarehouseModel:
        """The warehouse used for executing cached SQL queries."""
        return self.parameters.warehouse

    @property
    def time_to_live(self) -> timedelta:
        """Time-to-live for cache entries."""
        return timedelta(seconds=self.parameters.time_to_live_seconds)

    @property
    def similarity_threshold(self) -> float:
        """Minimum cosine similarity for cache hit."""
        return self.parameters.similarity_threshold

    @property
    def embedding_dims(self) -> int:
        """Dimension size for embeddings (auto-detected if not configured)."""
        if self._embedding_dims is None:
            raise RuntimeError(
                "Embedding dimensions not yet initialized. Call _setup() first."
            )
        return self._embedding_dims

    @property
    def table_name(self) -> str:
        """Name of the cache table."""
        return self.parameters.table_name

    @property
    def cleanup_probability(self) -> float:
        """Probability of cleaning expired entries on write (0.0-1.0)."""
        return self.parameters.cleanup_probability

    def _create_table_if_not_exists(self) -> None:
        """Create the cache table with pg_vector extension if it doesn't exist."""
        create_extension_sql: str = "CREATE EXTENSION IF NOT EXISTS vector"
        create_table_sql: str = f"""
            CREATE TABLE IF NOT EXISTS {self.table_name} (
                id SERIAL PRIMARY KEY,
                genie_space_id TEXT NOT NULL,
                question TEXT NOT NULL,
                question_embedding vector({self.embedding_dims}),
                sql_query TEXT NOT NULL,
                description TEXT,
                conversation_id TEXT,
                created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
            )
        """
        # Index for efficient similarity search partitioned by genie_space_id
        create_embedding_index_sql: str = f"""
            CREATE INDEX IF NOT EXISTS {self.table_name}_embedding_idx 
            ON {self.table_name} 
            USING ivfflat (question_embedding vector_cosine_ops)
            WITH (lists = 100)
        """
        # Index for filtering by genie_space_id
        create_space_index_sql: str = f"""
            CREATE INDEX IF NOT EXISTS {self.table_name}_space_idx 
            ON {self.table_name} (genie_space_id)
        """

        with self._pool.connection() as conn:
            with conn.cursor() as cur:
                cur.execute(create_extension_sql)
                cur.execute(create_table_sql)
                cur.execute(create_space_index_sql)
                # Only create ivfflat index if table has enough rows
                cur.execute(f"SELECT COUNT(*) FROM {self.table_name}")
                row: tuple[Any, ...] | dict[str, Any] | None = cur.fetchone()
                # Handle both dict-like and tuple-like row access
                row_count: int
                if row is None:
                    row_count = 0
                elif isinstance(row, dict):
                    row_count = row.get("count", 0)
                else:
                    row_count = row[0]
                if row_count >= 100:
                    try:
                        cur.execute(create_embedding_index_sql)
                    except Exception as e:
                        logger.debug(
                            f"[{self.name}] Embedding index creation skipped: {e}"
                        )

    def _embed_question(self, question: str) -> list[float]:
        """Generate embedding for a question."""
        embeddings: list[list[float]] = self._embeddings.embed_documents([question])
        return embeddings[0]

    @mlflow.trace(name="semantic_search", span_type=SpanType.TOOL)
    def _find_similar(
        self, question: str, embedding: list[float]
    ) -> tuple[SQLCacheEntry, float] | None:
        """
        Find a semantically similar cached entry for this Genie space.

        Args:
            question: The question to search for
            embedding: The embedding vector of the question

        Returns:
            Tuple of (SQLCacheEntry, similarity_score) if found, None otherwise
        """
        # Use cosine similarity: 1 - cosine_distance
        # pg_vector's <=> operator returns cosine distance (0 = identical, 2 = opposite)
        # Filter by genie_space_id to ensure cache partitioning
        search_sql: str = f"""
            SELECT 
                question,
                sql_query,
                description,
                conversation_id,
                created_at,
                1 - (question_embedding <=> %s::vector) as similarity
            FROM {self.table_name}
            WHERE genie_space_id = %s
              AND created_at > NOW() - INTERVAL '%s seconds'
            ORDER BY question_embedding <=> %s::vector
            LIMIT 1
        """

        embedding_str: str = f"[{','.join(str(x) for x in embedding)}]"
        ttl_seconds: int = self.parameters.time_to_live_seconds

        with self._pool.connection() as conn:
            with conn.cursor() as cur:
                cur.execute(
                    search_sql,
                    (embedding_str, self.genie_space_id, ttl_seconds, embedding_str),
                )
                row: tuple[Any, ...] | dict[str, Any] | None = cur.fetchone()

                if row is None:
                    logger.info(
                        f"[{self.name}] No cached entries found for space '{self.genie_space_id}' "
                        f"(threshold={self.similarity_threshold}, ttl={ttl_seconds}s)"
                    )
                    mlflow.update_current_trace(
                        tags={
                            "cache_layer": self.name,
                            "semantic_search_result": "no_entries",
                            "similarity_threshold": str(self.similarity_threshold),
                            "genie_space_id": self.genie_space_id,
                        }
                    )
                    return None

                # Access by index since we may not have dict_row
                similarity: float
                cached_question: str
                entry: SQLCacheEntry
                if isinstance(row, dict):
                    similarity = row["similarity"]
                    cached_question = row.get("question", "")
                    if similarity < self.similarity_threshold:
                        logger.info(
                            f"[{self.name}] Best match similarity={similarity:.4f} < threshold={self.similarity_threshold} "
                            f"(cached_question='{cached_question[:50]}...')"
                        )
                        mlflow.update_current_trace(
                            tags={
                                "cache_layer": self.name,
                                "semantic_search_result": "below_threshold",
                                "similarity_score": f"{similarity:.4f}",
                                "similarity_threshold": str(self.similarity_threshold),
                                "cached_question_preview": cached_question[:50],
                            }
                        )
                        return None

                    entry = SQLCacheEntry(
                        query=row["sql_query"],
                        description=row["description"] or "",
                        conversation_id=row["conversation_id"] or "",
                        created_at=row["created_at"],
                    )
                else:
                    # Tuple access (question, sql_query, description, conversation_id, created_at, similarity)
                    similarity = row[5]
                    cached_question = row[0] if row[0] else ""
                    if similarity < self.similarity_threshold:
                        logger.info(
                            f"[{self.name}] Best match similarity={similarity:.4f} < threshold={self.similarity_threshold} "
                            f"(cached_question='{cached_question[:50]}...')"
                        )
                        mlflow.update_current_trace(
                            tags={
                                "cache_layer": self.name,
                                "semantic_search_result": "below_threshold",
                                "similarity_score": f"{similarity:.4f}",
                                "similarity_threshold": str(self.similarity_threshold),
                                "cached_question_preview": cached_question[:50],
                            }
                        )
                        return None

                    entry = SQLCacheEntry(
                        query=row[1],
                        description=row[2] or "",
                        conversation_id=row[3] or "",
                        created_at=row[4],
                    )

                logger.info(
                    f"[{self.name}] Semantic match found: similarity={similarity:.4f} >= threshold={self.similarity_threshold} "
                    f"(cached_question='{cached_question[:50]}...')"
                )
                mlflow.update_current_trace(
                    tags={
                        "cache_layer": self.name,
                        "semantic_search_result": "hit",
                        "similarity_score": f"{similarity:.4f}",
                        "similarity_threshold": str(self.similarity_threshold),
                        "cached_question_preview": cached_question[:50],
                    }
                )

                return entry, similarity

    def _store_entry(
        self, question: str, embedding: list[float], response: GenieResponse
    ) -> None:
        """Store a new cache entry for this Genie space.

        Includes probabilistic cleanup of expired entries to prevent unbounded table growth.
        Cleanup runs with probability defined by cleanup_probability parameter (default 10%).
        """
        import random

        insert_sql: str = f"""
            INSERT INTO {self.table_name} 
            (genie_space_id, question, question_embedding, sql_query, description, conversation_id)
            VALUES (%s, %s, %s::vector, %s, %s, %s)
        """
        embedding_str: str = f"[{','.join(str(x) for x in embedding)}]"

        with self._pool.connection() as conn:
            with conn.cursor() as cur:
                # Probabilistic cleanup of expired entries
                if random.random() < self.parameters.cleanup_probability:
                    cleanup_sql: str = f"""
                        DELETE FROM {self.table_name}
                        WHERE genie_space_id = %s
                          AND created_at < NOW() - INTERVAL '%s seconds'
                    """
                    ttl_seconds: int = self.parameters.time_to_live_seconds
                    cur.execute(cleanup_sql, (self.genie_space_id, ttl_seconds))
                    deleted: int = cur.rowcount
                    logger.info(
                        f"[{self.name}] On-write cleanup: pruned {deleted} expired entries "
                        f"(space={self.genie_space_id}, ttl={ttl_seconds}s)"
                    )
                    # Add to MLflow trace for observability
                    mlflow.update_current_trace(
                        tags={
                            "cleanup_triggered": "true",
                            "cleanup_entries_pruned": str(deleted),
                        }
                    )

                # Insert the new entry
                cur.execute(
                    insert_sql,
                    (
                        self.genie_space_id,
                        question,
                        embedding_str,
                        response.query,
                        response.description,
                        response.conversation_id,
                    ),
                )
                logger.info(
                    f"[{self.name}] Stored cache entry: question='{question[:50]}...' "
                    f"sql='{response.query[:50]}...' space={self.genie_space_id}"
                )

    @mlflow.trace(name="execute_cached_sql_semantic", span_type=SpanType.TOOL)
    def _execute_sql(self, sql: str) -> pd.DataFrame | str:
        """Execute SQL using the warehouse and return results."""
        mlflow.update_current_trace(
            tags={
                "cache_layer": self.name,
                "warehouse_id": self.warehouse.warehouse_id or "",
            }
        )

        client: WorkspaceClient = self.warehouse.workspace_client
        warehouse_id: str = self.warehouse.warehouse_id

        statement_response: StatementResponse = (
            client.statement_execution.execute_statement(
                warehouse_id=warehouse_id,
                statement=sql,
                wait_timeout="30s",
            )
        )

        if statement_response.status.state != StatementState.SUCCEEDED:
            error_msg: str = (
                f"SQL execution failed: {statement_response.status.error.message}"
                if statement_response.status.error
                else f"SQL execution failed with state: {statement_response.status.state}"
            )
            logger.error(f"[{self.name}] {error_msg}")
            return error_msg

        if statement_response.result and statement_response.result.data_array:
            columns: list[str] = []
            if (
                statement_response.manifest
                and statement_response.manifest.schema
                and statement_response.manifest.schema.columns
            ):
                columns = [
                    col.name for col in statement_response.manifest.schema.columns
                ]
            elif hasattr(statement_response.result, "schema"):
                columns = [col.name for col in statement_response.result.schema.columns]

            data: list[list[Any]] = statement_response.result.data_array
            if columns:
                return pd.DataFrame(data, columns=columns)
            else:
                return pd.DataFrame(data)

        return pd.DataFrame()

    def ask_question(
        self, question: str, conversation_id: str | None = None
    ) -> GenieResponse:
        """
        Ask a question, using semantic cache if a similar query exists.

        On cache hit, re-executes the cached SQL to get fresh data.
        Implements GenieServiceBase for seamless chaining.
        """
        result: CacheResult = self.ask_question_with_cache_info(
            question, conversation_id
        )
        return result.response

    @mlflow.trace(name="genie_semantic_cache_lookup", span_type=SpanType.TOOL)
    def ask_question_with_cache_info(
        self,
        question: str,
        conversation_id: str | None = None,
        skip_cache: bool = False,
    ) -> CacheResult:
        """
        Ask a question with detailed cache hit information.

        On cache hit, the cached SQL is re-executed to return fresh data.

        Args:
            question: The question to ask
            conversation_id: Optional conversation ID
            skip_cache: If True, bypass this cache (still queries downstream)

        Returns:
            CacheResult with fresh response and cache metadata
        """
        # Ensure setup is complete
        self._setup()

        # Generate embedding for the question
        embedding: list[float] = self._embed_question(question)

        # Check cache unless skipping
        if not skip_cache:
            cache_result: tuple[SQLCacheEntry, float] | None = self._find_similar(
                question, embedding
            )

            if cache_result is not None:
                cached, similarity = cache_result
                logger.debug(
                    f"[{self.name}] Semantic cache hit (similarity={similarity:.3f}): {question[:50]}..."
                )

                # Calculate cache entry age
                cache_age: timedelta = (
                    datetime.now(cached.created_at.tzinfo) - cached.created_at
                )
                cache_age_seconds: float = cache_age.total_seconds()

                # Log cache hit to trace with TTL and timing info
                mlflow.update_current_trace(
                    tags={
                        "cache_hit": "true",
                        "cache_layer": self.name,
                        "similarity_score": f"{similarity:.4f}",
                        "cached_sql": cached.query[:200] if cached.query else "",
                        "cache_entry_created_at": cached.created_at.isoformat(),
                        "cache_entry_age_seconds": str(int(cache_age_seconds)),
                        "cache_ttl_seconds": str(
                            int(self.time_to_live.total_seconds())
                        ),
                        "cache_ttl_remaining_seconds": str(
                            int(self.time_to_live.total_seconds() - cache_age_seconds)
                        ),
                    }
                )

                # Re-execute the cached SQL to get fresh data
                result: pd.DataFrame | str = self._execute_sql(cached.query)

                response: GenieResponse = GenieResponse(
                    result=result,
                    query=cached.query,
                    description=cached.description,
                    conversation_id=cached.conversation_id,
                )

                return CacheResult(
                    response=response, cache_hit=True, served_by=self.name
                )

        # Cache miss - delegate to wrapped service
        logger.debug(
            f"[{self.name}] {'Skip' if skip_cache else 'Miss'}: {question[:50]}..."
        )

        # Log cache miss to trace with TTL info
        mlflow.update_current_trace(
            tags={
                "cache_hit": "false",
                "cache_layer": self.name,
                "skip_cache": str(skip_cache),
                "cache_ttl_seconds": str(int(self.time_to_live.total_seconds())),
            }
        )

        response = self.impl.ask_question(question, conversation_id)

        # Store in cache if we got a SQL query
        if response.query and not skip_cache:
            logger.info(
                f"[{self.name}] Storing new cache entry for question: '{question[:50]}...' "
                f"(space={self.genie_space_id})"
            )
            self._store_entry(question, embedding, response)
        elif not response.query:
            logger.warning(
                f"[{self.name}] Not caching: response has no SQL query "
                f"(question='{question[:50]}...')"
            )

        return CacheResult(response=response, cache_hit=False, served_by=None)

    def invalidate_expired(self) -> int:
        """Remove expired entries from the cache for this Genie space."""
        self._setup()
        delete_sql: str = f"""
            DELETE FROM {self.table_name}
            WHERE genie_space_id = %s
              AND created_at < NOW() - INTERVAL '%s seconds'
        """
        ttl_seconds: int = self.parameters.time_to_live_seconds

        with self._pool.connection() as conn:
            with conn.cursor() as cur:
                cur.execute(delete_sql, (self.genie_space_id, ttl_seconds))
                deleted: int = cur.rowcount
                logger.debug(
                    f"[{self.name}] Deleted {deleted} expired entries for space {self.genie_space_id}"
                )
                return deleted

    def clear(self) -> int:
        """Clear all entries from the cache for this Genie space."""
        self._setup()
        delete_sql: str = f"DELETE FROM {self.table_name} WHERE genie_space_id = %s"

        with self._pool.connection() as conn:
            with conn.cursor() as cur:
                cur.execute(delete_sql, (self.genie_space_id,))
                deleted: int = cur.rowcount
                logger.debug(
                    f"[{self.name}] Cleared {deleted} entries for space {self.genie_space_id}"
                )
                return deleted

    @property
    def size(self) -> int:
        """Current number of entries in the cache for this Genie space."""
        self._setup()
        count_sql: str = (
            f"SELECT COUNT(*) FROM {self.table_name} WHERE genie_space_id = %s"
        )

        with self._pool.connection() as conn:
            with conn.cursor() as cur:
                cur.execute(count_sql, (self.genie_space_id,))
                row: tuple[Any, ...] | dict[str, Any] | None = cur.fetchone()
                if row is None:
                    return 0
                elif isinstance(row, dict):
                    return row.get("count", 0)
                else:
                    return row[0]

    def stats(self) -> dict[str, int | float]:
        """Return cache statistics for this Genie space."""
        self._setup()
        stats_sql: str = f"""
            SELECT
                COUNT(*) as total,
                COUNT(*) FILTER (WHERE created_at > NOW() - INTERVAL '%s seconds') as valid,
                COUNT(*) FILTER (WHERE created_at <= NOW() - INTERVAL '%s seconds') as expired
            FROM {self.table_name}
            WHERE genie_space_id = %s
        """
        ttl_seconds: int = self.parameters.time_to_live_seconds

        with self._pool.connection() as conn:
            with conn.cursor() as cur:
                cur.execute(stats_sql, (ttl_seconds, ttl_seconds, self.genie_space_id))
                row: tuple[Any, ...] | dict[str, Any] | None = cur.fetchone()
                if row:
                    if isinstance(row, dict):
                        return {
                            "size": row["total"],
                            "ttl_seconds": self.time_to_live.total_seconds(),
                            "similarity_threshold": self.similarity_threshold,
                            "expired_entries": row["expired"],
                            "valid_entries": row["valid"],
                        }
                    else:
                        return {
                            "size": row[0],
                            "ttl_seconds": self.time_to_live.total_seconds(),
                            "similarity_threshold": self.similarity_threshold,
                            "expired_entries": row[2],
                            "valid_entries": row[1],
                        }
                return {
                    "size": 0,
                    "ttl_seconds": self.time_to_live.total_seconds(),
                    "similarity_threshold": self.similarity_threshold,
                    "expired_entries": 0,
                    "valid_entries": 0,
                }
